# StudyMate: AI-Powered PDF-Based Q&A System for Students
# Designed to run in Google Colab with Hugging Face integration

# Install required packages
!pip install transformers torch sentence-transformers PyPDF2 faiss-cpu gradio huggingface_hub

import os
import PyPDF2
import numpy as np
import pandas as pd
import torch
from sentence_transformers import SentenceTransformer
import faiss
from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering
import gradio as gr
from huggingface_hub import login
import re
from typing import List, Tuple, Dict
import warnings
warnings.filterwarnings('ignore')

class StudyMateQA:
    def __init__(self, hf_token: str = None):
        """Initialize the StudyMate Q&A system"""
        self.hf_token = hf_token
        self.documents = []
        self.embeddings = None
        self.index = None
        self.embedding_model = None
        self.qa_pipeline = None
        
        # Login to Hugging Face if token provided
        if hf_token:
            login(token=hf_token)
            
        self.setup_models()
    
    def setup_models(self):
        """Initialize the embedding and QA models"""
        print("Loading embedding model...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        print("Loading question-answering model...")
        self.qa_pipeline = pipeline(
            "question-answering",
            model="distilbert-base-cased-distilled-squad",
            tokenizer="distilbert-base-cased-distilled-squad"
        )
        
        print("Models loaded successfully!")
    
    def extract_text_from_pdf(self, pdf_file) -> str:
        """Extract text from uploaded PDF file"""
        try:
            if hasattr(pdf_file, 'read'):
                # File object
                reader = PyPDF2.PdfReader(pdf_file)
            else:
                # File path
                with open(pdf_file, 'rb') as file:
                    reader = PyPDF2.PdfReader(file)
            
            text = ""
            for page in reader.pages:
                text += page.extract_text() + "\n"
            
            return text.strip()
        except Exception as e:
            return f"Error extracting text from PDF: {str(e)}"
    
    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """Split text into overlapping chunks for better context preservation"""
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = " ".join(words[i:i + chunk_size])
            if chunk.strip():
                chunks.append(chunk.strip())
        
        return chunks
    
    def process_pdf(self, pdf_file, progress=gr.Progress()) -> str:
        """Process uploaded PDF and create embeddings"""
        try:
            progress(0, desc="Extracting text from PDF...")
            text = self.extract_text_from_pdf(pdf_file)
            
            if text.startswith("Error"):
                return text
            
            progress(0.3, desc="Chunking text...")
            chunks = self.chunk_text(text)
            
            if not chunks:
                return "No text could be extracted from the PDF."
            
            progress(0.5, desc="Creating embeddings...")
            embeddings = self.embedding_model.encode(chunks)
            
            progress(0.8, desc="Building search index...")
            # Create FAISS index
            self.index = faiss.IndexFlatIP(embeddings.shape[1])
            self.index.add(embeddings.astype('float32'))
            
            # Store documents and embeddings
            self.documents = chunks
            self.embeddings = embeddings
            
            progress(1.0, desc="Complete!")
            
            return f"✅ PDF processed successfully!\n📄 Extracted {len(chunks)} text chunks\n📊 Ready for questions!"
            
        except Exception as e:
            return f"Error processing PDF: {str(e)}"
    
    def retrieve_relevant_chunks(self, question: str, top_k: int = 3) -> List[str]:
        """Retrieve most relevant text chunks for the question"""
        if self.index is None or not self.documents:
            return []
        
        # Create question embedding
        question_embedding = self.embedding_model.encode([question])
        
        # Search for similar chunks
        scores, indices = self.index.search(question_embedding.astype('float32'), top_k)
        
        # Return relevant chunks
        relevant_chunks = [self.documents[idx] for idx in indices[0]]
        return relevant_chunks
    
    def answer_question(self, question: str) -> Tuple[str, float, List[str]]:
        """Generate answer for the given question"""
        if not question.strip():
            return "Please ask a question.", 0.0, []
        
        if self.index is None or not self.documents:
            return "Please upload and process a PDF first.", 0.0, []
        
        try:
            # Retrieve relevant chunks
            relevant_chunks = self.retrieve_relevant_chunks(question, top_k=3)
            
            if not relevant_chunks:
                return "No relevant information found in the document.", 0.0, []
            
            # Combine chunks for context
            context = " ".join(relevant_chunks)
            
            # Truncate context if too long
            max_context_length = 1000
            if len(context) > max_context_length:
                context = context[:max_context_length] + "..."
            
            # Generate answer using QA pipeline
            result = self.qa_pipeline(question=question, context=context)
            
            answer = result['answer']
            confidence = result['score']
            
            return answer, confidence, relevant_chunks
            
        except Exception as e:
            return f"Error generating answer: {str(e)}", 0.0, []
    
    def create_interface(self):
        """Create Gradio interface for the Q&A system"""
        
        def process_and_answer(pdf_file, question):
            if pdf_file is None:
                return "Please upload a PDF file first.", "", ""
            
            # Process PDF if not already processed
            if self.index is None:
                process_result = self.process_pdf(pdf_file)
                if not process_result.startswith("✅"):
                    return process_result, "", ""
            
            if not question.strip():
                return "PDF processed. Please ask a question.", "", ""
            
            # Generate answer
            answer, confidence, sources = self.answer_question(question)
            
            # Format confidence
            confidence_text = f"Confidence: {confidence:.2%}" if confidence > 0 else ""
            
            # Format sources
            sources_text = ""
            if sources:
                sources_text = "📚 **Relevant Text Sections:**\n\n"
                for i, source in enumerate(sources[:2], 1):
                    preview = source[:200] + "..." if len(source) > 200 else source
                    sources_text += f"**Section {i}:** {preview}\n\n"
            
            return answer, confidence_text, sources_text
        
        # Create Gradio interface
        with gr.Blocks(title="StudyMate: AI-Powered PDF Q&A", theme=gr.themes.Soft()) as interface:
            gr.Markdown("""
            # 📚 StudyMate: AI-Powered PDF Q&A System
            
            Upload your study materials (PDF) and ask questions to get instant answers!
            Perfect for studying textbooks, research papers, and lecture notes.
            """)
            
            with gr.Row():
                with gr.Column(scale=1):
                    pdf_input = gr.File(
                        label="📄 Upload PDF Document",
                        file_types=[".pdf"],
                        type="binary"
                    )
                    
                    question_input = gr.Textbox(
                        label="❓ Ask a Question",
                        placeholder="What is the main topic discussed in this document?",
                        lines=2
                    )
                    
                    submit_btn = gr.Button("🔍 Get Answer", variant="primary")
                
                with gr.Column(scale=2):
                    answer_output = gr.Textbox(
                        label="💡 Answer",
                        lines=4,
                        interactive=False
                    )
                    
                    confidence_output = gr.Textbox(
                        label="📊 Confidence Score",
                        lines=1,
                        interactive=False
                    )
                    
                    sources_output = gr.Markdown(
                        label="📚 Source Context",
                        value=""
                    )
            
            # Example questions
            gr.Markdown("""
            ### 💡 Example Questions:
            - "What is the main argument of this paper?"
            - "Summarize the key findings in chapter 3"
            - "What are the advantages and disadvantages mentioned?"
            - "Define [specific term] as used in this document"
            - "What methodology was used in this study?"
            """)
            
            # Event handlers
            submit_btn.click(
                fn=process_and_answer,
                inputs=[pdf_input, question_input],
                outputs=[answer_output, confidence_output, sources_output]
            )
            
            question_input.submit(
                fn=process_and_answer,
                inputs=[pdf_input, question_input],
                outputs=[answer_output, confidence_output, sources_output]
            )
        
        return interface

# Main execution function
def main():
    """Main function to run StudyMate"""
    
    print("🚀 Initializing StudyMate Q&A System...")
    
    # Get Hugging Face token (optional but recommended)
    hf_token = input("Enter your Hugging Face token (optional, press Enter to skip): ").strip()
    if not hf_token:
        hf_token = None
        print("⚠️ Running without Hugging Face token. Some models may not be accessible.")
    
    # Initialize StudyMate
    studymate = StudyMateQA(hf_token=hf_token)
    
    # Create and launch interface
    print("🎯 Creating user interface...")
    interface = studymate.create_interface()
    
    # Launch the app
    print("🌟 Launching StudyMate...")
    interface.launch(
        share=True,  # Creates public link
        debug=False,
        height=800,
        show_error=True
    )

# Alternative: Command line interface for testing
def test_studymate():
    """Simple command line test function"""
    
    print("Testing StudyMate Q&A System...")
    
    # Initialize without token for testing
    studymate = StudyMateQA()
    
    # Test with sample text (simulating PDF content)
    sample_text = """
    Artificial Intelligence (AI) is a branch of computer science that aims to create 
    intelligent machines that work and react like humans. Machine Learning is a subset 
    of AI that provides systems the ability to automatically learn and improve from 
    experience without being explicitly programmed. Deep Learning is a subset of 
    Machine Learning that uses neural networks with multiple layers to model and 
    understand complex patterns in data.
    """
    
    # Simulate processing
    chunks = studymate.chunk_text(sample_text)
    embeddings = studymate.embedding_model.encode(chunks)
    studymate.index = faiss.IndexFlatIP(embeddings.shape[1])
    studymate.index.add(embeddings.astype('float32'))
    studymate.documents = chunks
    
    # Test questions
    questions = [
        "What is Artificial Intelligence?",
        "What is Machine Learning?",
        "How is Deep Learning related to AI?"
    ]
    
    for question in questions:
        answer, confidence, sources = studymate.answer_question(question)
        print(f"\nQ: {question}")
        print(f"A: {answer}")
        print(f"Confidence: {confidence:.2%}")

# Run the application
if __name__ == "__main__":
    # Choose mode: 'main' for full interface, 'test' for command line testing
    mode = input("Choose mode - 'main' for full interface or 'test' for quick test (default: main): ").strip().lower()
    
    if mode == 'test':
        test_studymate()
    else:
        main()